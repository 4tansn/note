import requests
from bs4 import BeautifulSoup
from langchain.schema import Document

# Confluence REST API setup
CONFLUENCE_BASE_URL = "https://your-confluence-domain.atlassian.net/wiki/rest/api/content/"
API_USERNAME = "your-email"
API_TOKEN = "your-api-token"

# Function to fetch Confluence page content
def fetch_confluence_page(page_id):
    url = f"{CONFLUENCE_BASE_URL}{page_id}?expand=body.view"
    response = requests.get(url, auth=(API_USERNAME, API_TOKEN))
    if response.status_code == 200:
        data = response.json()
        return data['body']['view']['value']
    else:
        print(f"Error fetching page {page_id}: {response.status_code}")
        return None

# Function to extract paragraphs as documents from the Confluence page
def extract_documents_from_html(html_content, page_id):
    soup = BeautifulSoup(html_content, "lxml")  # Parse the HTML using BeautifulSoup
    text = soup.get_text()  # Get all text from the page
    lines = text.splitlines()  # Split the content into lines based on new lines

    # Filter out any empty lines
    lines = [line.strip() for line in lines if line.strip()]

    documents = []
    temp_content = []
    
    # Iterate through lines and group them into sets of 4 lines per document
    for line in lines:
        temp_content.append(line)

        # Once we have 4 lines, create a document
        if len(temp_content) == 4:
            document_text = "\n".join(temp_content)
            metadata = {"page_id": page_id}
            document = Document(page_content=document_text, metadata=metadata)
            documents.append(document)
            temp_content = []  # Clear temp_content for the next 4 lines
    
    # Add any remaining lines as a final document if not empty
    if temp_content:
        document_text = "\n".join(temp_content)
        metadata = {"page_id": page_id}
        document = Document(page_content=document_text, metadata=metadata)
        documents.append(document)
    
    return documents

# Function to fetch and process a Confluence page as LangChain Documents
def scrape_confluence_page_to_documents(page_id):
    page_content = fetch_confluence_page(page_id)
    if page_content:
        documents = extract_documents_from_html(page_content, page_id)
        return documents
    return []

# Example usage
page_id = "123456"  # Replace with your actual Confluence page ID
documents = scrape_confluence_page_to_documents(page_id)

# Output the list of documents
print("Extracted LangChain Documents (4 lines per document):")
for i, doc in enumerate(documents, 1):
    print(f"Document {i}:")
    print(f"Content: {doc.page_content}")
    print(f"Metadata: {doc.metadata}")
    print("-" * 40)
