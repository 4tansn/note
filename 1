import requests
from bs4 import BeautifulSoup
from langchain.docstore.document import Document
import tiktoken

# Define a tokenizer for the target model (e.g., GPT-3, LLaMA)
tokenizer = tiktoken.get_encoding("gpt2")  # Choose the appropriate tokenizer

# Function to scrape web page content
def scrape_web_page(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")

    # Extract text, paragraphs, tables, and URLs (modify as needed)
    text_content = []
    
    for paragraph in soup.find_all("p"):
        text_content.append(paragraph.get_text())

    for table in soup.find_all("table"):
        text_content.append(table.get_text())

    for link in soup.find_all("a", href=True):
        text_content.append(f"Link: {link['href']} - Text: {link.get_text()}")

    return " ".join(text_content)

# Function to tokenize and split content into Langchain documents
def create_langchain_documents(content, max_tokens=512):
    tokens = tokenizer.encode(content)
    
    # Split tokens into chunks of max_tokens size
    documents = []
    for i in range(0, len(tokens), max_tokens):
        chunk = tokens[i:i + max_tokens]
        chunk_text = tokenizer.decode(chunk)
        documents.append(Document(page_content=chunk_text))
    
    return documents

# Example usage
url = "https://example.com"  # Replace with the target URL
content = scrape_web_page(url)
documents = create_langchain_documents(content)

# Print the Langchain documents
for i, doc in enumerate(documents):
    print(f"Document {i+1}:")
    print(doc.page_content)
    print("="*50)
