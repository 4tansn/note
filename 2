from sentence_transformers import SentenceTransformer
from langchain.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings.base import Embeddings

# Custom Embedding class for Sentence Transformers
class SentenceTransformerEmbeddings(Embeddings):
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.model = SentenceTransformer(model_name)

    def embed_documents(self, documents):
        return self.model.encode(documents, convert_to_tensor=True).cpu().numpy()

    def embed_query(self, query):
        return self.model.encode([query], convert_to_tensor=True).cpu().numpy()[0]

# Initialize Sentence Transformer Embeddings
embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")

# Function to vectorize text data
def vectorize_text(text_data, embeddings):
    text_splitter = CharacterTextSplitter(separator="\n", chunk_size=1000, chunk_overlap=0)
    texts = text_splitter.split_text(text_data)
    return embeddings.embed_documents(texts)

# Save vectorized data to local disk using FAISS
def save_vectors_to_disk(vectors, file_path="vector_data.faiss"):
    faiss_index = FAISS.from_embeddings(vectors)
    faiss_index.save_local(file_path)

# Example usage
if __name__ == "__main__":
    # Sample text data (replace this with your scraped Confluence data)
    all_page_content = "Here is some sample text from a Confluence page."

    # Vectorize the scraped data
    vectors = vectorize_text(all_page_content, embeddings)

    # Save vectors to disk
    save_vectors_to_disk(vectors)
    
    print("Vectors saved locally.")
